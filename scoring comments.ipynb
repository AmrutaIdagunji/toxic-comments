{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb988de3",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "672058da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import scipy.optimize as optimize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "#pd.options.display.max_colwidth=300\n",
    "#pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ac3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e18071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7898c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78bec82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b61ea",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d112937",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"train.csv\")\n",
    "validation_data = pd.read_csv(\"validation_data.csv\")\n",
    "testing_data = pd.read_csv(\"comments_to_score.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b423ea78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...    0.0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...    0.0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...    0.0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...    0.0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...    0.0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  sum_score  \n",
       "0           0.0      0.0     0.0     0.0            0.0        0.0  \n",
       "1           0.0      0.0     0.0     0.0            0.0        0.0  \n",
       "2           0.0      0.0     0.0     0.0            0.0        0.0  \n",
       "3           0.0      0.0     0.0     0.0            0.0        0.0  \n",
       "4           0.0      0.0     0.0     0.0            0.0        0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales = {'obscene': 0.16, 'toxic': 0.32, 'insult': 0.64, 'threat': 1.5, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n",
    "\n",
    "for category in scales:\n",
    "    training_data[category] = training_data[category] * scales[category]\n",
    "\n",
    "training_data['sum_score'] = training_data.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n",
    "\n",
    "training_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d87166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = testing_data.rename(columns = {'text': 'comment_text'}, inplace = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8c7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2e9253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.score = [val for val in df['sum_score']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['comment_text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.score)\n",
    "\n",
    "    def get_score(self, idx):\n",
    "        return np.array(self.score[idx])\n",
    "    \n",
    "    def get_batch_texts(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_score = self.get_score(idx)\n",
    "\n",
    "        return batch_texts, batch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05abd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944c28de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127656 15957 15958\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(training_data[['comment_text','sum_score']].sample(frac=1, random_state=42), [int(.8*len(training_data[['comment_text','sum_score']])), int(.9*len(training_data[['comment_text','sum_score']]))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4312339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bc44975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.MSELoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_score in tqdm(train_dataloader):\n",
    "\n",
    "                train_score = train_score.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_score)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_score).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_score in val_dataloader:\n",
    "\n",
    "                    val_score = val_score.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_score)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_score).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4dc02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LR = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c064e7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñç                                                                       | 336/63828 [1:16:55<242:17:09, 13.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     35\u001b[0m     total_acc_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n\u001b[0;32m     37\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     41\u001b[0m total_acc_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\amruta\\anaconda3\\lib\\site-packages\\torch\\tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    238\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    239\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    244\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\amruta\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 145\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45107b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_score in test_dataloader:\n",
    "\n",
    "              test_label = test_score.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_score).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    \n",
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597722f2",
   "metadata": {},
   "source": [
    "## Slightly different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\n",
    "X = tfidf_vec.fit_transform(training_data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.5)\n",
    "model.fit(X, training_data['sum_score'])\n",
    "l_model = Ridge(alpha=1.)\n",
    "l_model.fit(X, training_data['sum_score'])\n",
    "s_model = Ridge(alpha=2.)\n",
    "s_model.fit(X, training_data['sum_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07358473",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['less_toxic cleaning'] = validation_data['less_toxic'].apply(text_cleaning)\n",
    "validation_data['more_toxic cleaning'] = validation_data['more_toxic'].apply(text_cleaning)\n",
    "\n",
    "X_less_toxic = tfidf_vec.transform(validation_data['less_toxic cleaning'])\n",
    "X_more_toxic = tfidf_vec.transform(validation_data['more_toxic cleaning'])\n",
    "\n",
    "validation_data['less_toxic score cleaning'] = model.predict(X_less_toxic)\n",
    "validation_data['more_toxic score cleaning'] = model.predict(X_more_toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['cleaning text'] = testing_data['text'].apply(text_cleaning)\n",
    "text_vector = tfidf_vec.transform(testing_data['cleaning text'])\n",
    "\n",
    "testing_data['score_05'] = model.predict(text_vector)\n",
    "testing_data['score_10'] = l_model.predict(text_vector)\n",
    "testing_data['score_15'] = s_model.predict(text_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Accuracy\n",
    "print(f'val : {(validation_data[\"less_toxic score cleaning\"] < validation_data[\"more_toxic score cleaning\"]).mean()}')\n",
    "\n",
    "testing_data['score2'] = (testing_data['score_05'] + testing_data['score_10'] + testing_data['score_15']) / 3.\n",
    "#testing_data[['comment_id', 'avg score']].to_csv(\"submission3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data[\"score\"] = .66*testing_data[\"score1\"] + .44*testing_data[\"score2\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data[\"score\"] = rankdata(testing_data[\"score\"], method='ordinal')\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f685686",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['finalscore'] = 0\n",
    "for i in range(0, 500):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 1.35\n",
    "for i in range(801, 1200):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 1.45\n",
    "for i in range(1701, 2300):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 0.81\n",
    "for i in range(2501, 2980):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 0.85    \n",
    "for i in range(3001, 4000):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 1.42    \n",
    "for i in range(4001, 4500):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 1.45   \n",
    "for i in range(4501, 4940):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 0.86\n",
    "for i in range(5501, 5980):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 0.83\n",
    "for i in range(6001, 6500):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 1.45\n",
    "for i in range(7001, 7536):\n",
    "    testing_data['finalscore'].iloc[i] = testing_data['score'].iloc[i] * 1.42 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfaaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data[\"finalscore\"] = rankdata(testing_data[\"finalscore\"], method='ordinal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['finalscore'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data[['comment_id', 'finalscore']].sort_values(by = 'finalscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data[['comment_id', 'finalscore']].to_csv('submissionfinal.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
